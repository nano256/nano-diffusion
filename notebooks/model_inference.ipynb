{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Model inference\n",
    "This notebook allows you to load model checkpoints and use them to generate images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusion.model import NanoDiffusionModel\n",
    "from diffusion.utils import CosineNoiseScheduler, DDIMSampler, decode_latents, get_available_device\n",
    "from diffusers.models import AutoencoderKL\n",
    "import matplotlib.pyplot as plt\n",
    "from ipyfilechooser import FileChooser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = get_available_device()\n",
    "LABEL_NAMES = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FileChooser(\n",
    "    path='../models/',  # Start directory\n",
    "    filename='',\n",
    "    title='Select a model checkpoint:',\n",
    "    # show_only_dirs=True  # Only show directories\n",
    ")\n",
    "\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = fc.selected\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "\n",
    "model_config = checkpoint[\"model_config\"]\n",
    "noise_scheduler_config = checkpoint[\"noise_scheduler_config\"]\n",
    "model = NanoDiffusionModel(model_config).to(DEVICE).eval()\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(model_config.vae_name).to(DEVICE).eval()\n",
    "noise_scheduler = CosineNoiseScheduler(noise_scheduler_config)\n",
    "\n",
    "sampler = DDIMSampler(model, noise_scheduler, noise_scheduler_config.num_timesteps, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Sample all classes\n",
    "Generate an image for each class of CIFAR-10 and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(10, 16, 4, 4).to(DEVICE)\n",
    "labels = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1).to(DEVICE)\n",
    "latents = sampler.sample(noise, labels)\n",
    "\n",
    "images = decode_latents(latents, vae)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (img, label) in enumerate(zip(images, labels)):\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"{LABEL_NAMES[label]}\", fontsize=14)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(\"Generated CIFAR-10 Samples\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Intermediate generation steps\n",
    "This time, we convert all steps of the sampling process into the image space. This allows us to see how the model denoises the image over the different timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "timesteps = list(range(1000, -1, -100))\n",
    "\n",
    "noise = torch.randn(10, 16, 4, 4).to(DEVICE)\n",
    "labels = torch.tensor([labels]).reshape(-1).to(DEVICE)\n",
    "final_latents, intermediates = sampler.sample(noise, labels, return_intermediates=True)\n",
    "\n",
    "# Select 11 samples from each noise trajectory \n",
    "intermediates = [noise] + intermediates\n",
    "latents = intermediates[::5] + [final_latents]\n",
    "latents = torch.stack(latents)\n",
    "latents_shape = latents.shape\n",
    "\n",
    "# Get the samples in the right order for the plot\n",
    "latents = latents.transpose(0, 1).reshape(-1, *latents.shape[2:])\n",
    "\n",
    "images = decode_latents(latents, vae)\n",
    "\n",
    "fig, axes = plt.subplots(10, 11, figsize=(12, 10))\n",
    "\n",
    "for row in range(10):\n",
    "    for col in range(11):\n",
    "        idx = row * 11 + col\n",
    "        axes[row, col].imshow(images[idx])\n",
    "\n",
    "        if row == 0:\n",
    "            axes[row, col].set_title(f\"t={timesteps[col]}\", fontsize=8, pad=2)\n",
    "\n",
    "        if col == 0:\n",
    "            axes[row, col].set_ylabel(LABEL_NAMES[row], fontsize=8, rotation=0, labelpad=35)\n",
    "            # Hide ticks, otherwise it messes up the layout\n",
    "            axes[row, col].set_xticks([])\n",
    "            axes[row, col].set_yticks([])\n",
    "            axes[row, col].spines['top'].set_visible(False)\n",
    "            axes[row, col].spines['right'].set_visible(False)\n",
    "            axes[row, col].spines['bottom'].set_visible(False)\n",
    "            axes[row, col].spines['left'].set_visible(False)\n",
    "\n",
    "        if col != 0:\n",
    "              axes[row, col].axis('off')\n",
    "\n",
    "plt.suptitle(\"Denoising trajectories\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nano-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
