{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Model inference\n",
    "This notebook allows you to load model checkpoints and use them to generate images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from diffusion.model import NanoDiffusionModel\n",
    "from diffusion.utils import CosineNoiseScheduler, DDIMSampler, decode_latents, get_available_device\n",
    "from diffusers.models import AutoencoderKL\n",
    "import matplotlib.pyplot as plt\n",
    "from ipyfilechooser import FileChooser\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = get_available_device()\n",
    "LABEL_NAMES = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FileChooser(\n",
    "    path='../models/',  # Start directory\n",
    "    filename='',\n",
    "    title='Select a model checkpoint:',\n",
    "    # show_only_dirs=True  # Only show directories\n",
    ")\n",
    "\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = fc.selected\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "\n",
    "model_config = checkpoint[\"model_config\"]\n",
    "noise_scheduler_config = checkpoint[\"noise_scheduler_config\"]\n",
    "model = NanoDiffusionModel(model_config).to(DEVICE).eval()\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(model_config.vae_name).to(DEVICE).eval()\n",
    "noise_scheduler = CosineNoiseScheduler(noise_scheduler_config)\n",
    "\n",
    "sampler = DDIMSampler(model, noise_scheduler, noise_scheduler_config.num_timesteps, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Sample all classes\n",
    "Generate an image for each class of CIFAR-10 and display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(10, 16, 4, 4).to(DEVICE)\n",
    "classes = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).reshape(-1).to(DEVICE)\n",
    "latents = sampler.sample(noise, classes)\n",
    "\n",
    "images = decode_latents(latents, vae)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (img, class_label) in enumerate(zip(images, classes)):\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"{LABEL_NAMES[class_label]}\", fontsize=14)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(\"Generated CIFAR-10 Samples\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Intermediate generation steps\n",
    "This time, we convert all steps of the sampling process into the image space. This allows us to see how the model denoises the image over the different timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn(1, 16, 4, 4).to(DEVICE)\n",
    "classes = torch.tensor([0]).reshape(-1).to(DEVICE)\n",
    "latent, intermediates = sampler.sample(noise, classes, return_intermediates=True)\n",
    "\n",
    "images = decode_latents(latents, vae)\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (img, class_label) in enumerate(zip(images, classes)):\n",
    "    axes[idx].imshow(img)\n",
    "    axes[idx].set_title(f\"{LABEL_NAMES[class_label]}\", fontsize=14)\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle(\"Generated CIFAR-10 Samples\", fontsize=16, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nano-diffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
